{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s39Os91Zj4pp40HlsI7Sojh8z7oZnebS",
      "authorship_tag": "ABX9TyPKXpCd9J1t9QupnHoZHUo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flaaa31/meetings_handling/blob/main/meetings_handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Riassunto e Identificazione delle Attività dalla Trascrizione di un Meeting\n",
        "## Introduzione\n",
        "La capacità di analizzare le trascrizioni dei meeting aziendali è fondamentale per organizzare il lavoro e identificare le azioni chiave. In questo progetto, lo studente implementerà un sistema basato su modelli di linguaggio di grandi dimensioni (LLM) come GPT o LLama per processare una trascrizione e generare un riassunto e un elenco strutturato delle attività.\n",
        "\n",
        "## Obiettivo del Progetto\n",
        "Realizzare uno script in Python che:\n",
        "1. Analizzi una trascrizione di un meeting fornita (disponibile al link: https://raw.githubusercontent.com/Profession-AI/progetti-llm/refs/heads/main/Riassunto%20e%20identificazione%20delle%20attivit%C3%A0%20dalla%20trascrizione%20di%20un%20meeting/meeting_transcription.txt).\n",
        "2. Generi un riassunto conciso dei punti principali discussi.\n",
        "3. Identifichi e strutturi le attività assegnate a ciascun partecipante.\n",
        "\n",
        "## Requisiti\n",
        "* Input: Un file di testo contenente la trascrizione del meeting (es. meeting_transcription.txt).\n",
        "* Output:\n",
        "  * Un riassunto dei punti chiave del meeting.\n",
        "  * Un elenco di attività assegnate, con indicazione della persona responsabile.\n",
        "\n",
        "## Linee Guida Tecniche\n",
        "1. Scelta del Modello:\n",
        "\n",
        "  * Utilizzare un modello di linguaggio LLM come GPT o LLama. Si consiglia l'uso di API di OpenAI o un'implementazione open-source di LLaMA.\n",
        "2. Pipeline del Progetto:\n",
        "\n",
        "  * Pre-processing: Caricare il file di trascrizione.\n",
        "  * Riassunto: Utilizzare il modello LLM per generare un riassunto dei contenuti principali.\n",
        "  * Estrazione delle Attività: Analizzare il testo per identificare frasi che indicano compiti assegnati, azioni future o responsabilità.\n",
        "3. Struttura dell'Output:\n",
        "\n",
        "  * Riassunto:\n",
        "  Riassunto del Meeting:\n",
        "    - Punto 1...\n",
        "    - Punto 2...\n",
        "  * Elenco delle Attività:\n",
        "  Attività Identificate:\n",
        "    - Mario Rossi: Redigere un piano tecnico basato sui requisiti emersi.\n",
        "    - Andrea Monti: Implementare un modulo per le notifiche e il server SMTP.\n",
        "4. Testing e Validazione:\n",
        "\n",
        "  * Verificare che il sistema sia in grado di identificare correttamente attività e responsabilità, con un livello di accuratezza accettabile.\n",
        "\n",
        "## Consegna\n",
        "* Uno script Python documentato.\n",
        "* Il riassunto generato e l'elenco delle attività basati sul file di trascrizione fornito.\n",
        "* Un breve report che descriva:\n",
        "  * Il funzionamento dello script.\n",
        "  * I risultati ottenuti.\n",
        "  * Le eventuali limitazioni del sistema."
      ],
      "metadata": {
        "id": "2S0DRoMlK8VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup"
      ],
      "metadata": {
        "id": "y6uy0r_PNPSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain llama_index dotenv --q\n",
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "id": "Nw_9H9yxmA2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b04f55-acf8-4859-990a-d28482fd990e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading GROQ_API_KEY, used for LLama3 model, the only LLM used in this project\n",
        "it is free (with usage limitations) api keys can be created [here](https://console.groq.com/keys)\n"
      ],
      "metadata": {
        "id": "9FQmU_pLH3D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading GROQ_API_KEY, used for LLama3 model (only LLM used in this project), using a .env file\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(\"/content/drive/MyDrive/ProfessionAI/Master AI Developer/7.LLM/progetto/.env\") # better to create a .env file, instead of showing api keys in the code, for safety reasons\n",
        "\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "jqkBKGwmm_mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meeting file download\n",
        "! wget \"https://raw.githubusercontent.com/Profession-AI/progetti-llm/refs/heads/main/Riassunto%20e%20identificazione%20delle%20attivit%C3%A0%20dalla%20trascrizione%20di%20un%20meeting/meeting_transcription.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj_3YE5H3n7C",
        "outputId": "f9680a06-3f4c-457b-fba0-343dc1268808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-15 07:44:12--  https://raw.githubusercontent.com/Profession-AI/progetti-llm/refs/heads/main/Riassunto%20e%20identificazione%20delle%20attivit%C3%A0%20dalla%20trascrizione%20di%20un%20meeting/meeting_transcription.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2518 (2.5K) [text/plain]\n",
            "Saving to: ‘meeting_transcription.txt’\n",
            "\n",
            "meeting_transcripti 100%[===================>]   2.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-15 07:44:12 (39.1 MB/s) - ‘meeting_transcription.txt’ saved [2518/2518]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# every file in the current folder will be included in \"documents\" list, using SimpleDirectoryReader\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "folder= '.' # current directory, where we download \"meeting_transcription.txt\"\n",
        "\n",
        "# Loading documents\n",
        "documents = SimpleDirectoryReader(folder).load_data()"
      ],
      "metadata": {
        "id": "BB03OO6j4ZVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only one document in our folder, no need to worry about choosing the wrong file\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEF2yj6-E1ir",
        "outputId": "134fa488-1a9a-4a8d-8e69-6e45f96a5358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing first and only document\n",
        "print(documents[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U3popr1-7o0",
        "outputId": "ad94b5a0-8469-490e-edbe-07e62d01a84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mario Rossi: Buongiorno a tutti. Come sapete, siamo qui per discutere i requisiti del nuovo sistema contabile. Rossana, potresti iniziare spiegandoci le principali necessità dal punto di vista dell’utente?\n",
            "\n",
            "Rossana Bolletta: Certo, Mario. Attualmente il nostro sistema è molto macchinoso. Abbiamo bisogno di un'interfaccia più intuitiva per l’inserimento delle fatture, una gestione automatizzata delle scadenze dei pagamenti e una reportistica personalizzabile.\n",
            "\n",
            "Mario Rossi: Capisco. Quando parli di reportistica personalizzabile, intendi la possibilità di scegliere i parametri da analizzare, come intervalli di tempo o categorie di spesa?\n",
            "\n",
            "Rossana Bolletta: Esatto. Al momento siamo costretti a estrarre i dati e modificarli manualmente in Excel. Sarebbe ideale poter generare i report direttamente dal sistema, filtrando secondo le nostre esigenze.\n",
            "\n",
            "Andrea Monti: Questo implica la necessità di un modulo dedicato ai filtri e ai grafici. Inoltre, per l’automazione delle scadenze, immagino servano notifiche o promemoria?\n",
            "\n",
            "Rossana Bolletta: Sì, promemoria per le scadenze delle fatture sarebbe perfetto. Sarebbe utile anche poter inviare email automatiche ai fornitori in caso di ritardi nei pagamenti.\n",
            "\n",
            "Mario Rossi: Andrea, possiamo integrare un sistema di notifiche e invio email nella piattaforma?\n",
            "\n",
            "Andrea Monti: Sì, non è un problema. Potremmo utilizzare una libreria per la gestione delle notifiche e configurare un server SMTP per l’invio delle email. Rossana, ci sono altre funzioni che ritieni essenziali?\n",
            "\n",
            "Rossana Bolletta: Sì, una cosa che ci manca è un’archiviazione digitale dei documenti. Sarebbe utile poter allegare PDF delle fatture direttamente nel sistema.\n",
            "\n",
            "Mario Rossi: Andrea, puoi prendere nota di questo. Avremo bisogno di uno spazio di storage e di una gestione documentale.\n",
            "\n",
            "Andrea Monti: Certo. Pensavo di utilizzare un servizio cloud come base per l’archiviazione. Potremmo implementare una funzione di ricerca avanzata per trovare rapidamente i documenti.\n",
            "\n",
            "Mario Rossi: Bene, mi sembra che abbiamo un quadro chiaro delle necessità. Rossana, c'è altro che vorresti aggiungere?\n",
            "\n",
            "Rossana Bolletta: No, credo di aver elencato tutto. Grazie per aver ascoltato le nostre esigenze.\n",
            "\n",
            "Mario Rossi: Perfetto. Andrea, ti occupi di redigere un piano tecnico basato sui requisiti emersi. Poi faremo un altro incontro per approvare il progetto definitivo.\n",
            "\n",
            "Andrea Monti: Va bene, mi metto subito al lavoro.\n",
            "\n",
            "Mario Rossi: Grazie a tutti per la partecipazione. Buona giornata!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating meeting summary and tasks using Langchain"
      ],
      "metadata": {
        "id": "WTAJOPmhO0Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meeting_text = documents[0].text"
      ],
      "metadata": {
        "id": "qXmKtRpifuUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition, in our case llama3-70b-8192, but we can use other models if we want\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llama3 = ChatGroq(\n",
        "    api_key = GROQ_API_KEY,\n",
        "    model_name = \"llama3-70b-8192\")\n",
        "\n",
        "\"\"\"\n",
        "# we could also use openai models, for example gpt-4o-mini but we need a paid account and generating the api_key from their site (https://platform.openai.com/api-keys)\n",
        "gpt4o = ChatOpenAI(\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    model_name = \"gpt-4o-mini\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tqZgN5q3OCZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "212a9046-dc5d-48c9-b752-d2bdf41baafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# we could also use openai models, for example gpt-4o-mini but we need a paid account and generating the api_key from their site (https://platform.openai.com/api-keys)\\ngpt4o = ChatOpenAI(\\n    api_key = os.environ.get(\"OPENAI_API_KEY\"),\\n    model_name = \"gpt-4o-mini\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero shot example, no meeting file given, expecting it will say the deafult answer\n",
        "messages = [\n",
        "    (\n",
        "      \"system\",\n",
        "      \"Sei un chatbot specializzato nell'aiutare gli utenti con delle informazioni sui meeting, se non viene data nessuna informazione sul meeting, rispondi solamente con: 'Scusa, non ho ancora ricevuto alcuna discussione, quindi non posso riassumere nulla'\",\n",
        "    ),\n",
        "    (\n",
        "     \"human\",\n",
        "     \"Parlami brevemente del meeting che è stato fatto.\"\n",
        "     )]\n",
        "\n",
        "llama_zeroshot = llama3.invoke(messages)\n",
        "\n",
        "print(llama_zeroshot.content)"
      ],
      "metadata": {
        "id": "e7kh11jkO45W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87951e38-2b5e-4519-efe4-4725db2f9e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scusa, non ho ancora ricevuto alcuna discussione, quindi non posso riassumere nulla.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without any file given, the model correctly says that there's no file, and it doesn't make any allucinations"
      ],
      "metadata": {
        "id": "2XAsDkZagnPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's see if the model \"sees\" the meeting file"
      ],
      "metadata": {
        "id": "5tPgexxCom-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "# Generic prompt template\n",
        "PROMPT_MEETING = \"\"\"\\\n",
        "Sei un chatbot specializzato nell'aiutare gli utenti con delle informazioni sui meeting. \\\n",
        "Se non viene data nessuna informazione sul meeting, rispondi solamente con: \\\n",
        "'Scusa, non ho ancora ricevuto alcuna discussione, quindi non posso riassumere nulla'.\n",
        "\n",
        "Ecco il testo del meeting:\n",
        "'''{meeting_text}'''\n",
        "\"\"\"\n",
        "\n",
        "prompt_meeting = PromptTemplate(\n",
        "    input_variables=[\"meeting_text\"],\n",
        "    template=PROMPT_MEETING\n",
        ")\n",
        "\n",
        "# Building the runnable chain: prompt | model | output parsing as string\n",
        "meeting_chain = prompt_meeting | llama3 | StrOutputParser()\n",
        "\n",
        "# Chain invocation, passing meeting_text\n",
        "result = meeting_chain.invoke({\"meeting_text\": meeting_text})\n",
        "\n",
        "print(result)  # model response\n"
      ],
      "metadata": {
        "id": "UggS3cKA-3nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f300bd8-bca4-423c-f9d6-ae3304dec51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ecco il riassunto del meeting:\n",
            "\n",
            "Il meeting è stato convocato per discutere i requisiti del nuovo sistema contabile. I principali requisiti emersi sono:\n",
            "\n",
            "* Un'interfaccia più intuitiva per l’inserimento delle fatture\n",
            "* Una gestione automatizzata delle scadenze dei pagamenti\n",
            "* Una reportistica personalizzabile\n",
            "* Un modulo dedicato ai filtri e ai grafici\n",
            "* Notifiche e promemoria per le scadenze delle fatture\n",
            "* Invio email automatiche ai fornitori in caso di ritardi nei pagamenti\n",
            "* Un'archiviazione digitale dei documenti con possibilità di allegare PDF delle fatture\n",
            "* Una gestione documentale con funzione di ricerca avanzata\n",
            "\n",
            "Andrea Monti si occuperà di redigere un piano tecnico basato sui requisiti emersi e ci sarà un altro incontro per approvare il progetto definitivo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model talks about the correct discussion file, without any allucinations"
      ],
      "metadata": {
        "id": "JaB4IsdVRxKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary generation and task definition\n",
        "- task1: meeting summary\n",
        "- task2: tasks extraction for every person"
      ],
      "metadata": {
        "id": "y03VBzr7mgZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "# Task 1: meeting summary\n",
        "PROMPT_SUMMARY = \"\"\"\\\n",
        "Sei un chatbot specializzato nel riassumere i meeting.\n",
        "Genera un riassunto conciso dei punti principali discussi nel meeting:\n",
        "\n",
        "L'output deve avere il seguente formato:\n",
        "Riassunto del Meeting:\n",
        "- Punto 1...\n",
        "- Punto 2...\n",
        "\n",
        "Non scrivere nulla prima o dopo l'output.\n",
        "Ecco il testo del meeting:\n",
        "'''{meeting_text}'''\n",
        "\"\"\"\n",
        "\n",
        "prompt_summary = PromptTemplate(\n",
        "    input_variables=[\"meeting_text\"],\n",
        "    template=PROMPT_SUMMARY\n",
        ")\n",
        "\n",
        "# Task 2: tasks extraction\n",
        "PROMPT_TASKS = \"\"\"\\\n",
        "Partendo da questo meeting: {meeting_text}, identifica e struttura tutte le attività assegnate ai partecipanti.\n",
        "L'output deve avere il seguente formato:\n",
        "Attività Identificate:\n",
        "- Partecipante 1: Attività (descrizione sintetica)...\n",
        "- Partecipante 2: Attività (descrizione sintetica)...\n",
        "\n",
        "Se il partecipante non ha task assegnati, metti \"Nessun task\" come descrizione.\n",
        "Non scrivere nulla prima o dopo l'output.\n",
        "\"\"\"\n",
        "\n",
        "prompt_tasks = PromptTemplate(\n",
        "    input_variables=[\"meeting_text\"],\n",
        "    template=PROMPT_TASKS\n",
        ")\n",
        "\n",
        "# Building two chains, one for summary and one for tasks\n",
        "chain_summary = prompt_summary | llama3 | StrOutputParser()\n",
        "chain_tasks = prompt_tasks | llama3 | StrOutputParser()\n",
        "\n",
        "\n",
        "# Invoking of two chains\n",
        "result_1 = chain_summary.invoke({\"meeting_text\": meeting_text})\n",
        "result_2 = chain_tasks.invoke({\"meeting_text\": meeting_text})\n",
        "\n",
        "# Printing both Results\n",
        "print(\"Riassunto:\\n\", result_1)\n",
        "print(\"\\nAttività:\\n\", result_2)\n",
        "\n",
        "# Saving results as a markdown_file\n",
        "final_result = result_1 + \"\\n\\n\" + result_2\n",
        "with open(\"final_result.md\", \"w\") as f:\n",
        "    f.write(final_result)\n",
        "\n",
        "# display saving message\n",
        "print(\"\\nResults saved as 'final_result.md'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBwYDx0plJWo",
        "outputId": "dc83c457-989d-4666-ad9c-a04dc8d68393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riassunto:\n",
            " Riassunto del Meeting:\n",
            "- Il nuovo sistema contabile deve avere un'interfaccia più intuitiva per l’inserimento delle fatture.\n",
            "- È necessaria una gestione automatizzata delle scadenze dei pagamenti con notifiche e promemoria.\n",
            "- La reportistica deve essere personalizzabile con la possibilità di scegliere i parametri da analizzare.\n",
            "- È richiesta una funzione di archiviazione digitale dei documenti con allegazione di PDF delle fatture.\n",
            "- Sarà necessario uno spazio di storage e una gestione documentale con funzione di ricerca avanzata.\n",
            "- Andrea Monti sarà responsabile della redazione di un piano basato sui requisiti emersi.\n",
            "\n",
            "Attività:\n",
            " Attività Identificate:\n",
            "- Rossana Bolletta: Nessun task\n",
            "- Andrea Monti: Integrare sistema di notifiche e invio email, implementare funzione di ricerca avanzata per documenti, redigere piano tecnico\n",
            "- Mario Rossi: Nessun task\n",
            "\n",
            "Results saved as 'final_result.md'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final solution, in a more professional way\n",
        "main function that:\n",
        "- download discussion file from URL, or load it from a local file\n",
        "- query to LLM model, handling big files and too many requests in a short amount of time (chunk division and retry with exponential time)\n",
        "- summary of the discussion meeting\n",
        "- tasks extraction for every person in the meeting\n",
        "- translates everything in a language of choice (italian, in this case)\n",
        "- display and saves results in a markdown file\n",
        "\n",
        "For this case, I generated using ChatGPT a long meeting file (\"long_meeting.txt\"), it doesn't make that much sense, it starts talking about a data scientist meeting and then it goes on and on with other stuff to test the chunking of text that is too large for the LLM context window, so the content will not make that much sense, but it's only for educational purposes\n",
        "\n",
        "It can work also with the previous meeting file, if we set the \"is_url\" parameter to True"
      ],
      "metadata": {
        "id": "-fjLFbAQ-C6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries import\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import textwrap\n",
        "import time\n",
        "import random\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# API configuration\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# API URL\n",
        "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "\n",
        "def query_llama(prompt, max_tokens=4000, temperature=0.1, max_retries=5, initial_delay=2):\n",
        "    \"\"\"\n",
        "    Function that calls the LLama3-70b model using Groq with exponential backoff and retry.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    prompt(str): Prompt to give\n",
        "    max_tokens(int): Maximum number of tokens to generate, default is 4000\n",
        "    temperature(float): Temperature for sampling, default is 0.1\n",
        "    max_retries(int): Maximum number of retries, default is 5\n",
        "    initial_delay(int): Initial delay in seconds before the first retry, default is 2\n",
        "\n",
        "    Returns:\n",
        "    str: Generated text\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\", # api_key for authentication\n",
        "        \"Content-Type\": \"application/json\" # to specify that we're gonna pass a json file as a body\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": \"llama3-70b-8192\", # the same model as before\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}], # chat structure\n",
        "        \"max_tokens\": max_tokens, # default max tokens (4000)\n",
        "        \"temperature\": temperature # default temperature (0.1), as low as we can in order to not obtain too creative responses\n",
        "    }\n",
        "\n",
        "    retry_count = 0 # how many times the request has been repeated\n",
        "    delay = initial_delay # waiting time before retry, starts from 2 by default\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            # Adding a delay before every API call to avoid rate limiting\n",
        "            if retry_count > 0:\n",
        "                print(f\"Waiting {delay} seconds before attempt {retry_count + 1}...\") # every retry, prints the delayed seconds\n",
        "            time.sleep(delay) # and wait that many seconds\n",
        "\n",
        "            response = requests.post(GROQ_API_URL, headers=headers, data=json.dumps(data)) # sending request to GROQ endpoint\n",
        "\n",
        "            # If we receive Status Code 429 (Too Many Requests), we wait and retry\n",
        "            if response.status_code == 429:\n",
        "                retry_count += 1\n",
        "                # Exponential backoff plus a random jitter to avoid synchronized peaks\n",
        "                delay = min(60, delay * 2) + random.uniform(0, 1)\n",
        "                print(f\"Rate limiting (429). Retry in {delay:.2f} seconds... (attempt {retry_count}/{max_retries})\")\n",
        "                continue\n",
        "\n",
        "            response.raise_for_status()  # Raise Exception for other HTTP Errors\n",
        "            return response.json()[\"choices\"][0][\"message\"][\"content\"] # returns generated JSON\n",
        "\n",
        "        except requests.exceptions.HTTPError as e: # HTTP exceptions handling\n",
        "            # printing error and body\n",
        "            print(f\"HTTP Error during API call: {e}\")\n",
        "            if 'response' in locals() and response:\n",
        "                print(f\"Server response: {response.text}\")\n",
        "\n",
        "            # If It's not 429 Error, or we have exhausted attempts, break cycle\n",
        "            if not (response.status_code == 429 and retry_count < max_retries):\n",
        "                break\n",
        "\n",
        "        except Exception as e: # Generic exceptions, same handling as before\n",
        "            print(f\"Error during API call: {e}\")\n",
        "            if 'response' in locals() and response:\n",
        "                print(f\"Server response: {response.text}\")\n",
        "            break\n",
        "\n",
        "        retry_count += 1 # incrementing retry count\n",
        "\n",
        "    print(f\"Impossible to complete request after {max_retries} attempts.\") # \"too many attempts\" error message\n",
        "    return None\n",
        "\n",
        "def download_discussion(url):\n",
        "    \"\"\"\n",
        "    Download discussion file from URL.\n",
        "\n",
        "    Parameters:\n",
        "    url(str): URL of the discussion file.\n",
        "\n",
        "    Returns:\n",
        "    str: Text content of the discussion file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during discussion download: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def split_text_into_chunks(text, max_chunk_size=4000):\n",
        "    \"\"\"\n",
        "    Split text into chunks of a given maximum size.\n",
        "\n",
        "    Parameters:\n",
        "    text(str): Text to split.\n",
        "    max_chunk_size(int): Maximum size of each chunk.\n",
        "\n",
        "    Returns:\n",
        "    list: List of chunks.\n",
        "    \"\"\"\n",
        "    # Dividing for each line\n",
        "    lines = text.split(\"\\n\")\n",
        "    chunks = [] # list to collect all the chunks\n",
        "    current_chunk = \"\" # temporary string to collect strings until max dimension is reached\n",
        "\n",
        "    for line in lines:\n",
        "        # If adding this line surpasses maximum length, we save the actual chunk\n",
        "        if len(current_chunk) + len(line) + 1 > max_chunk_size and current_chunk:\n",
        "            chunks.append(current_chunk) # adding current_chunk to chunks list\n",
        "            current_chunk = line  # starting new chunk with the line that surpassed the maximum length\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                current_chunk += \"\\n\" + line # if it doesn't surpass maximum length, we add the line to the current chunks list\n",
        "            else:\n",
        "                current_chunk = line # if current chunk is empty, we start with this line, without adding new lines\n",
        "\n",
        "    # If we have a last not empty current_chunk, we append it to chunks list\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Discussion divided in {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "def generate_summary_from_chunks(chunks, max_chunk_size=6000):\n",
        "    \"\"\"\n",
        "    Generate a summary from a list of chunks.\n",
        "\n",
        "    Parameters:\n",
        "    chunks(list): List of text chunks.\n",
        "    max_chunk_size(int): Maximum size of each chunk.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated summary.\n",
        "    \"\"\"\n",
        "    # First, we obtain partial results for every chunk\n",
        "    partial_summaries = [] # list of indivisual summaries of every chunk\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Elaboration of chunk {i+1}/{len(chunks)}...\")\n",
        "        prompt = f\"\"\"\\\n",
        "                  Sei un chatbot specializzato nel riassumere i meeting.\n",
        "                  Genera un riassunto conciso dei punti principali discussi in questa PARTE del meeting:\n",
        "                  '''{chunk}'''\n",
        "\n",
        "                  Tieni presente che questo è solo un frammento dell'intera discussione.\n",
        "                  Riassumi i punti chiave trattati in questa parte.\n",
        "                  \"\"\"\n",
        "\n",
        "        partial_summary = query_llama(prompt, temperature=0.2) # model calling\n",
        "        if partial_summary:\n",
        "            partial_summaries.append(partial_summary) # if we generate a summary, we append it to partial_summary\n",
        "            # Pause between chunk elaborations to avoid rate limiting\n",
        "            if i < len(chunks) - 1:  # No Pause after last chunk\n",
        "                pause_time = 2 + random.uniform(0, 1)  # Default Pause + jitter\n",
        "                print(f\"Waiting {pause_time:.2f} seconds before next chunk...\")\n",
        "                time.sleep(pause_time)\n",
        "\n",
        "    # Final summary obtained joining all partial summaries\n",
        "    if partial_summaries:\n",
        "        combined_summaries = \"\\n\\n\".join(partial_summaries)\n",
        "\n",
        "        # If combined_summaries is still too long, we have to divide it again\n",
        "        if len(combined_summaries) > max_chunk_size:\n",
        "            print(\"Combined Partial Summaries still too long, dividing again...\")\n",
        "            summary_chunks = split_text_into_chunks(combined_summaries, max_chunk_size) # break partial summaries in more chunks\n",
        "            final_summaries = []\n",
        "\n",
        "            # union of partial mini-summaries in final_summaries\n",
        "            for i, summary_chunk in enumerate(summary_chunks):\n",
        "                prompt = f\"\"\"\\\n",
        "                          Combina questi riassunti parziali in un unico riassunto coerente:\n",
        "                          '''{summary_chunk}'''\n",
        "\n",
        "                          L'output deve avere il seguente formato:\n",
        "                          Riassunto del Meeting:\n",
        "                          - Punto 1...\n",
        "                          - Punto 2...\n",
        "                          \"\"\"\n",
        "                # same model calling and appending\n",
        "                chunk_summary = query_llama(prompt, temperature=0.2)\n",
        "                if chunk_summary:\n",
        "                    final_summaries.append(chunk_summary)\n",
        "                # Pause between chunk elaborations to avoid rate limiting\n",
        "                if i < len(summary_chunks) - 1:  # No pause after last chunk\n",
        "                    pause_time = 2 + random.uniform(0, 1)\n",
        "                    print(f\"Waiting {pause_time:.2f} seconds before next chunk...\")\n",
        "                    time.sleep(pause_time)\n",
        "            #final summary obtained joining all partial mini-summaries\n",
        "            final_summary = \"\\n\".join(final_summaries)\n",
        "        else:\n",
        "            # If the size is manageable, let's make a single final summary\n",
        "            print(\"Generating final summary...\")\n",
        "            prompt = f\"\"\"\\\n",
        "                      Combina questi riassunti parziali in un unico riassunto coerente e conciso:\n",
        "                      '''{combined_summaries}'''\n",
        "\n",
        "                      L'output deve avere il seguente formato:\n",
        "                      Riassunto del Meeting:\n",
        "                      - Punto 1...\n",
        "                      - Punto 2...\n",
        "\n",
        "                      Non scrivere nulla prima o dopo l'output.\n",
        "                      \"\"\"\n",
        "\n",
        "            # Pause before final summary\n",
        "            pause_time = 3 + random.uniform(0, 2)\n",
        "            print(f\"Waiting {pause_time:.2f} seconds before final summary generation...\")\n",
        "            time.sleep(pause_time)\n",
        "\n",
        "            final_summary = query_llama(prompt, temperature=0.2)\n",
        "\n",
        "        return final_summary\n",
        "    else:\n",
        "        # no partial_summaries\n",
        "        return \"Impossible to generate a summary.\"\n",
        "\n",
        "def identify_tasks_from_chunks(chunks, max_chunk_size=6000):\n",
        "    \"\"\"\n",
        "    Identify tasks from a list of chunks.\n",
        "\n",
        "    Parameters:\n",
        "    chunks(list): List of text chunks.\n",
        "    max_chunk_size(int): Maximum size of each chunk.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated tasks.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect tasks from each chunk\n",
        "    all_tasks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Tasks extraction from chunk {i+1}/{len(chunks)}...\")\n",
        "        # prompt generation\n",
        "        prompt = f\"\"\"\n",
        "        Partendo da questa PARTE del meeting:\n",
        "        '''{chunk}'''\n",
        "\n",
        "        Identifica tutti i partecipanti presenti in questa parte e le attività assegnate ad essi.\n",
        "        Tieni presente che questo è solo un frammento dell'intera discussione.\n",
        "        Elenca solo le attività che trovi esplicitamente menzionate in questa parte.\n",
        "        \"\"\"\n",
        "\n",
        "        # model calling, always low temperature to avoid too much creativity\n",
        "        tasks = query_llama(prompt, temperature=0.2)\n",
        "        if tasks:\n",
        "            all_tasks.append(tasks) # appending tasks to the task list\n",
        "\n",
        "        # Pause between chunk elaborations to avoid rate limiting\n",
        "        if i < len(chunks) - 1:  # No pause after every chunk\n",
        "            pause_time = 2 + random.uniform(0, 1)\n",
        "            print(f\"Waiting {pause_time:.2f} seconds before next chunk...\")\n",
        "            time.sleep(pause_time)\n",
        "\n",
        "    # After processing all chunks, combine and deduplicate the tasks\n",
        "    if all_tasks:\n",
        "        combined_tasks = \"\\n\\n\".join(all_tasks)\n",
        "\n",
        "        # If combined activities are still too long, we have to further divide them\n",
        "        if len(combined_tasks) > max_chunk_size:\n",
        "            print(\"Combined activities still too long, dividing again...\")\n",
        "            # Split back into manageable chunks\n",
        "            tasks_chunks = split_text_into_chunks(combined_tasks, max_chunk_size)\n",
        "            final_tasks_list = []\n",
        "\n",
        "            # Process each split segment to deduplicate and format\n",
        "            for i, tasks_chunk in enumerate(tasks_chunks):\n",
        "                prompt = f\"\"\"\n",
        "                Combina e deduplicizza questo elenco di attività estratte da un meeting:\n",
        "                '''{tasks_chunk}'''\n",
        "\n",
        "                L'output deve avere il seguente formato:\n",
        "                Attività Identificate:\n",
        "                - Partecipante 1: Attività (descrizione sintetica) in italiano...\n",
        "                - Partecipante 2: Attività (descrizione sintetica) in italiano...\n",
        "\n",
        "                Elimina duplicati e attività simili. Mantieni solo informazioni uniche e rilevanti.\n",
        "                ASSICURATI CHE TUTTE LE ATTIVITÀ SIANO SCRITTE IN ITALIANO.\n",
        "                \"\"\"\n",
        "\n",
        "                # model calling\n",
        "                chunk_tasks = query_llama(prompt, temperature=0.2)\n",
        "                if chunk_tasks:\n",
        "                    final_tasks_list.append(chunk_tasks)\n",
        "\n",
        "                # Pause between chunk elaborations to avoid rate limiting\n",
        "                if i < len(tasks_chunks) - 1:\n",
        "                    pause_time = 2 + random.uniform(0, 1)\n",
        "                    print(f\"Waiting {pause_time:.2f} seconds before next chunk...\")\n",
        "                    time.sleep(pause_time)\n",
        "\n",
        "            final_tasks = \"\\n\".join(final_tasks_list)\n",
        "        else:\n",
        "            # If the size is manageable, we do a single deduplication step\n",
        "            print(\"Generating final tasks list...\")\n",
        "            prompt = f\"\"\"\n",
        "            Combina e deduplicizza questo elenco di attività estratte da un meeting:\n",
        "            '''{combined_tasks}'''\n",
        "\n",
        "            L'output deve avere il seguente formato:\n",
        "            Attività Identificate:\n",
        "            - Partecipante 1: Attività (descrizione sintetica) in italiano...\n",
        "            - Partecipante 2: Attività (descrizione sintetica) in italiano...\n",
        "\n",
        "            Elimina duplicati e attività simili. Mantieni solo informazioni uniche e rilevanti.\n",
        "            Se il partecipante non ha task assegnati, metti \"Nessun task\" come descrizione.\n",
        "            ASSICURATI CHE TUTTE LE ATTIVITÀ SIANO DESCRITTE IN ITALIANO, non lasciare frasi in inglese.\n",
        "            Non scrivere nulla prima o dopo l'output.\n",
        "            \"\"\"\n",
        "\n",
        "            # Pause before final task list generation\n",
        "            pause_time = 3 + random.uniform(0, 2)\n",
        "            print(f\"Waiting {pause_time:.2f} seconds before generating final tasks list...\")\n",
        "            time.sleep(pause_time)\n",
        "\n",
        "            # final model calling\n",
        "            final_tasks = query_llama(prompt, temperature=0.2)\n",
        "\n",
        "        return final_tasks\n",
        "    else:\n",
        "        # If no tasks were identified in any chunk, return a fallback message\n",
        "        return \"No tasks detected.\"\n",
        "\n",
        "# sometimes there are still language inconsistencies, so we define a translation funcion\n",
        "def translate_text(text, target_language):\n",
        "    \"\"\"\n",
        "    Translates text into the specified language using llama3 model.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): Text to be translated.\n",
        "        target_language (str): Target language code or name (e.g. 'it' or 'Italian').\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Traduci completamente il seguente testo in {target_language}.\n",
        "                È fondamentale che ogni frase e parola venga tradotta in {target_language}, anche i termini tecnici quando possibile.\n",
        "                Se ci sono termini tecnici che è meglio mantenere in inglese, mantienili ma traduci tutto il resto della frase.\n",
        "\n",
        "                Ecco il testo da tradurre:\n",
        "\n",
        "                {text}\n",
        "\n",
        "                Fornisci SOLO il testo tradotto in {target_language}, senza commenti aggiuntivi.\n",
        "              \"\"\"\n",
        "\n",
        "    # model calling for translation\n",
        "    translated_text = query_llama(prompt, temperature=0.2)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "def display_results(summary, tasks):\n",
        "    \"\"\"\n",
        "    Format and display results in Markdown format.\n",
        "\n",
        "    Parameters:\n",
        "    summary(str): Generated summary.\n",
        "    tasks(str): Generated tasks.\n",
        "\n",
        "    Returns:\n",
        "    str: Formatted Markdown output.\n",
        "    \"\"\"\n",
        "    # format of preference\n",
        "    markdown_output = f\"\"\"\n",
        "    # Analisi della Trascrizione del Meeting\n",
        "\n",
        "    ## Riassunto del Meeting\n",
        "    {summary}\n",
        "\n",
        "    ## Attività Identificate\n",
        "    {tasks}\n",
        "    \"\"\"\n",
        "\n",
        "    display(Markdown(markdown_output))\n",
        "\n",
        "    return markdown_output\n",
        "\n",
        "def save_results(output, filename=\"final_result.md\"):\n",
        "    \"\"\"\n",
        "    Save results to a file.\n",
        "\n",
        "    Parameters:\n",
        "    output(str): Formatted Markdown output.\n",
        "    filename(str): Name of the file to save, default is \"final_result.md\".\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(output)\n",
        "    print(f\"Results saved in '{filename}'\")\n",
        "\n",
        "def main(is_url = False):\n",
        "    \"\"\"\n",
        "    Main function to execute the analysis.\n",
        "    \"\"\"\n",
        "    # Maximum chunk (in characters) to avoid 413 Eroor\n",
        "    MAX_CHUNK_SIZE = 3000\n",
        "\n",
        "    if is_url:\n",
        "      transcript_url = \" https://raw.githubusercontent.com/Profession-AI/progetti-llm/refs/heads/main/Riassunto%20e%20identificazione%20delle%20attivit%C3%A0%20dalla%20trascrizione%20di%20un%20meeting/meeting_transcription.txt\"\n",
        "\n",
        "      print(\"Dowloading discussion file...\")\n",
        "      discussion = download_discussion(transcript_url)\n",
        "    else:\n",
        "      print(\"Reading local file...\")\n",
        "\n",
        "      # Reading local file content\n",
        "      try:\n",
        "          with open(\"meetings_folder/long_meeting.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "              discussion = f.read()\n",
        "      except Exception as e:\n",
        "          print(f\"Error in file reading: {e}\")\n",
        "          return\n",
        "\n",
        "\n",
        "    if not discussion:\n",
        "        print(\"Discussion file not found or empty\")\n",
        "        return\n",
        "\n",
        "    print(f\"Discussion loaded. Length: {len(discussion)} characters.\")\n",
        "    print(\"\\nFirst few lines:\")\n",
        "    print(textwrap.fill(discussion[:200] + \"...\", width=100))\n",
        "\n",
        "    # Dividing discussion in manageable chunks\n",
        "    chunks = split_text_into_chunks(discussion, MAX_CHUNK_SIZE)\n",
        "\n",
        "    print(\"\\nSummary generation from chunks...\")\n",
        "    summary = generate_summary_from_chunks(chunks)\n",
        "\n",
        "    print(\"\\nExtracting tasks from chunks...\")\n",
        "    tasks = identify_tasks_from_chunks(chunks)\n",
        "\n",
        "    # Original output\n",
        "    print(\"\\nAnalysis results:\")\n",
        "    output = display_results(summary, tasks)\n",
        "\n",
        "    # Translation in italian, to avoid some strange multi-lingual outputs\n",
        "    translated_output = translate_text(output, \"italiano\")\n",
        "    output_to_save = translated_output\n",
        "\n",
        "    # Saving results\n",
        "    save_results(output=output_to_save, filename=\"meeting_analysis_result.md\")\n",
        "\n",
        "    print(\"\\nAnalysis completed!\")\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mru_fj4Lysh8",
        "outputId": "c25616d9-f156-4d44-c8b7-395c1fa0e371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading local file...\n",
            "Discussion loaded. Length: 24783 characters.\n",
            "\n",
            "First few lines:\n",
            "Maria Rossi: Buongiorno a tutti. La riunione di oggi sarà un po' più lunga del solito: dobbiamo\n",
            "rivedere i risultati dei nuovi esperimenti, definire la versione alpha del modello da validare,\n",
            "discuter...\n",
            "Discussion divided in 9 chunks.\n",
            "\n",
            "Summary generation from chunks...\n",
            "Elaboration of chunk 1/9...\n",
            "Waiting 2.74 seconds before next chunk...\n",
            "Elaboration of chunk 2/9...\n",
            "Waiting 2.94 seconds before next chunk...\n",
            "Elaboration of chunk 3/9...\n",
            "Waiting 2.34 seconds before next chunk...\n",
            "Elaboration of chunk 4/9...\n",
            "Waiting 2.67 seconds before next chunk...\n",
            "Elaboration of chunk 5/9...\n",
            "Waiting 2.40 seconds before next chunk...\n",
            "Elaboration of chunk 6/9...\n",
            "Waiting 2.01 seconds before next chunk...\n",
            "Elaboration of chunk 7/9...\n",
            "Waiting 2.02 seconds before next chunk...\n",
            "Elaboration of chunk 8/9...\n",
            "Waiting 2.61 seconds before next chunk...\n",
            "Elaboration of chunk 9/9...\n",
            "Combined Partial Summaries still too long, dividing again...\n",
            "Discussion divided in 2 chunks.\n",
            "Rate limiting (429). Retry in 4.83 seconds... (attempt 1/5)\n",
            "Waiting 4.826244907646153 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 10.03 seconds... (attempt 2/5)\n",
            "Waiting 10.029877787954808 seconds before attempt 3...\n",
            "Waiting 2.87 seconds before next chunk...\n",
            "Rate limiting (429). Retry in 4.67 seconds... (attempt 1/5)\n",
            "Waiting 4.670598055553962 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 10.06 seconds... (attempt 2/5)\n",
            "Waiting 10.059063751989932 seconds before attempt 3...\n",
            "\n",
            "Extracting tasks from chunks...\n",
            "Tasks extraction from chunk 1/9...\n",
            "Rate limiting (429). Retry in 4.84 seconds... (attempt 1/5)\n",
            "Waiting 4.844690837957681 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 10.52 seconds... (attempt 2/5)\n",
            "Waiting 10.521225591089308 seconds before attempt 3...\n",
            "Waiting 2.80 seconds before next chunk...\n",
            "Tasks extraction from chunk 2/9...\n",
            "Waiting 2.06 seconds before next chunk...\n",
            "Tasks extraction from chunk 3/9...\n",
            "Rate limiting (429). Retry in 4.91 seconds... (attempt 1/5)\n",
            "Waiting 4.9122852913151025 seconds before attempt 2...\n",
            "Waiting 2.82 seconds before next chunk...\n",
            "Tasks extraction from chunk 4/9...\n",
            "Rate limiting (429). Retry in 4.85 seconds... (attempt 1/5)\n",
            "Waiting 4.848110970170566 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 10.02 seconds... (attempt 2/5)\n",
            "Waiting 10.02132776468765 seconds before attempt 3...\n",
            "Waiting 2.87 seconds before next chunk...\n",
            "Tasks extraction from chunk 5/9...\n",
            "Waiting 2.16 seconds before next chunk...\n",
            "Tasks extraction from chunk 6/9...\n",
            "Rate limiting (429). Retry in 4.92 seconds... (attempt 1/5)\n",
            "Waiting 4.916619280189775 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 10.39 seconds... (attempt 2/5)\n",
            "Waiting 10.387933804558646 seconds before attempt 3...\n",
            "Waiting 2.37 seconds before next chunk...\n",
            "Tasks extraction from chunk 7/9...\n",
            "Waiting 2.05 seconds before next chunk...\n",
            "Tasks extraction from chunk 8/9...\n",
            "Rate limiting (429). Retry in 4.99 seconds... (attempt 1/5)\n",
            "Waiting 4.985940298687486 seconds before attempt 2...\n",
            "Waiting 2.14 seconds before next chunk...\n",
            "Tasks extraction from chunk 9/9...\n",
            "Rate limiting (429). Retry in 4.32 seconds... (attempt 1/5)\n",
            "Waiting 4.3203040293601624 seconds before attempt 2...\n",
            "Combined activities still too long, dividing again...\n",
            "Discussion divided in 2 chunks.\n",
            "Rate limiting (429). Retry in 4.35 seconds... (attempt 1/5)\n",
            "Waiting 4.346299597880316 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 9.68 seconds... (attempt 2/5)\n",
            "Waiting 9.675276515175813 seconds before attempt 3...\n",
            "Rate limiting (429). Retry in 19.64 seconds... (attempt 3/5)\n",
            "Waiting 19.640646205893077 seconds before attempt 4...\n",
            "Waiting 2.89 seconds before next chunk...\n",
            "\n",
            "Analysis results:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n    # Analisi della Trascrizione del Meeting\n\n    ## Riassunto del Meeting\n    Ecco il riassunto coerente del meeting:\n\nRiassunto del Meeting:\n\n- Risultati esperimenti: Luca Bianchi ha presentato i risultati dei test della settimana, con varianti del modello LSTM, tra cui una con attenzione (attention layer) e una completamente trasformativa (Transformer). L'ibrido LSTM+attention si è comportato meglio, con RMSE 10.8 e varianza minore.\n\n- Problemi di overfitting e anomalie: Il problema principale resta l'overfitting sui dati di dicembre e gennaio, probabilmente dovuto a dati esterni (vacanze natalizie, black friday, eventi imprevedibili). Elena Verdi ha esplorato le anomalie nei dati tra novembre e gennaio, trovando picchi molto marcati in alcuni negozi, probabilmente dovuti a promozioni locali non registrate nei metadati.\n\n- Strategie per affrontare le anomalie: Sofia Gialli ha proposto di considerare una strategia a due stadi: prevedere se ci sarà un evento anomalo (binary classification), poi fare la regressione solo se la previsione è negativa. Giulio Ferri ha valutato che tecnicamente non ci sarebbero grandi complicazioni per implementare un sistema a due stadi.\n\n- Impatti sull'utente finale: Chiara Neri ha sollevato la questione degli impatti sul comportamento dell'utente finale, che dipenderanno da come gestiremo il messaging.\n\n- Intervallo di confidenza: Mostrare un intervallo di confidenza più largo in alcuni casi, addestrando una rete separata per stimare la varianza del prediction interval.\n\n- Explainability: Utilizzare Temporal SHAP per calcolare la contribuzione feature per timestep, mostrando i fattori chiave per ogni previsione. Batch explainability per le previsioni giornaliere, una volta al giorno.\n\n- Bias: Valutare il rischio di discriminazione territoriale, utilizzando tecniche di fairness-aware training come reweighting delle osservazioni o metodo di counterfactual fairness.\n\n- Storage: Cancella le versioni di modelli con performance inferiori alla baseline, tenendo solo i top 5 per ogni architettura.\n\n- Monitoraggio: Definire soglie chiare per il monitoraggio post-deployment, includendo latency, drift e outlier.\n\n- Cena di lavoro: Scegliere un ristorante informale per la cena di lavoro, con opzioni gluten-free e parcheggio gratuito.\n\n- Organizzazione della cena di squadra: Regali simbolici (tazze personalizzate con logo e nomi), punto d'incontro e modalità di arrivo.\n\n- Partecipazione all'hackathon interno: Presentazione di un proof-of-concept di previsione in 24 ore.\n\n- Definizione dei ruoli e delle responsabilità per l'hackathon: Marco Neri: ambiente Docker e predisposizione delle librerie. Elena Verdi: feature engineering e analisi esplorativa.\n\n- Modelli e esperimenti: Luca Bianchi: esperimenti con Transformer e modelli LSTM. Sofia Gialli: esperimenti zero-shot per feature automatiche. Giulio Ferri: setup Knative e monitoraggio.\n\n- Coordinamento e demo: Io e Chiara Neri: coordinamento, presentazione finale e demo.\n\n- Dataset e augmentation: Luca Bianchi: utilizzo di dataset sintetici per aumentare la variabilità. Marco Neri: creazione di un Dockerfile per testare in locale. Maria Rossi: adattamento del modulo di augmentation per serie temporali. Sofia Gialli: contributo al modulo augment.py con metodologie di bootstrapping e rolling window mixup.\n\n- Privacy e anonimizzazione: Giulio Ferri: anonimizzazione di pseudonimizzazione dei dati. Elena Verdi: notebook per mascherare nomi e indirizzi, da integrare nella pipeline.\n\n- Fairness e report legale: Chiara Neri: call con il reparto legale su GDPR e fairness. Elena Verdi: analisi delle distribuzioni regionali e heatmap. Sofia Gialli: metriche fairness disaggregate.\n\n- Altro: Marco Neri: problema di memory leak nel cluster GPU. Luca Bianchi: soluzione per chiudere la sessione manualmente in TensorFlow 2.10.\n\n- Costi cloud e ottimizzazione: Giulio Ferri propone di spegnere i nodi spot quando non servono e usare spot invece di on-demand per i test non urgenti per evitare di superare il budget.\n\n- Data drift detection: Sofia Gialli condivide la sua esperienza sul corso avanzato su MLOps di FaceAI e propone di utilizzare il test di Kolmogorov-Smirnov per identificare shift.\n\n- Demo interattiva del modello: Chiara Neri comunica che il management vuole una demo interattiva del modello nel portale aziendale entro fine mese.\n\n- Pianificato budget per il Q3.\nEcco il riassunto coerente del meeting:\n\nRiassunto del Meeting:\n\n- Budget e licenze: 11.000 € per formazione, 25.000 € per Weights & Biases, 4.000 € per Tableau, totale stimato 80.000 €, riserva per emergenze 20.000 €.\n- Roadmap di release: lancio della versione beta previsto per il 15 luglio, integrazione entro fine giugno, rollout su staging il 30 giugno, test utente a partire dal 1-5 luglio.\n- Copertura ferie estive: Chiara Neri in ferie dal 20 luglio al 3 agosto, Luca e Marco hanno il turno di copertura sulle emergenze.\n- KPI e Obiettivi: Elena Verdi raggiunge il target di data quality, Marco Neri raggiunge il target di tempo di latenza end-to-end della pipeline, Sofia Gialli raggiunge il target di adozione di nuove tecniche di ricerca, Giulio Ferri raggiunge il target di implementazione del monitoraggio end-to-end, Chiara Neri raggiunge parzialmente il target di time-to-market delle feature.\n- Azioni Correttive: incrementare il tasso di consolidamento degli esperimenti, ridurre l'impatto del GDPR sulla velocità dei report, analisi root-cause dei failure Airflow notturni, portare il prototipo GAN a validazione preliminare, raffinare la logica di alerting, ottimizzare il processo di QA.\n- Benessere e Smart Working: discussione sulla gestione del lavoro da remoto e in ufficio, proposte per migliorare il lavoro di squadra e il work-life balance, introduzione di un \"team wellbeing budget\" per voucher per yoga o palestra e sessioni di mindfulness in ufficio.\n- Lavoro ibrido e spazi di lavoro: proposta di lavoro da casa per la ricerca, modello ibrido 3/2 e spazio \"silent room\" per chi deve fare call o analisi senza distrazioni.\n- Diversity & Inclusion: questione della rappresentanza di background differenziati, proposte di iniziative come stage per neolaureati di università diverse e regioni svantaggiate, seminari con speaker esterni e partecipazione a conferenze sulla diversity, azioni da intraprendere come definire programma stage e criteri di selezione, organizzare webinar con speaker esterni e identificare conferenze D&I per partecipazione 2025.\n\n    ## Attività Identificate\n    Ecco l'elenco delle attività deduplicate e tradotte in italiano:\n\nAttività Identificate:\n- Maria Rossi: Coordinare le attività e prendere decisioni\n- Luca Bianchi: Presentare i risultati degli esperimenti e testare tecniche di fairness-aware training\n- Elena Verdi: Presentare le scoperte sulle anomalie nei dati e lavorare su metriche robuste per classificare i giorni come “anomalie”\n- Marco Neri: Verificare i log delle campagne digitali e predisporre un container Docker pre-configurato con tutte le librerie\n- Sofia Gialli: Suggerire di utilizzare un modello a due stadi per prevedere eventi anomali e provare un approccio zero-shot con GPT per generare feature a partire da documentazione testuale\n- Giulio Ferri: Valutare l'impatto tecnico di utilizzare un modello a due stadi sulla pipeline e implementare la funzione di auditing per loggare gli output “con zona modificata”\n- Chiara Neri: Valutare il rischio di discriminazione territoriale e utilizzare gli insight per motivare le previsioni nelle dashboard interne\n\nNota: ho eliminato le attività relative alla cena e all'hackathon, poiché non sembrano essere attività lavorative rilevanti. Inoltre, ho tradotto le attività in italiano e ho eliminato i duplicati.\nEcco l'elenco delle attività deduplicate e tradotte in italiano:\n\nAttività Identificate:\n- Giulio Ferri: Preparare un breve runbook operativo per il team e suggerire di investire in GPU dedicate on-premise se i costi cloud diventano troppo alti.\n- Sofia Gialli: Proof-of-concept data drift detection via KS test + autoencoder, proporre una licenza per la suite Weights & Biases e organizzare 3 webinar con speaker esterni entro luglio.\n- Marco Neri: Predisporre un bucket separato per i dati di validazione continua, analizzare la root-cause dei failure Airflow notturni e considerare una licenza enterprise di Tableau.\n- Luca Bianchi: Mettere in produzione un job schedulato con Airflow per calcolare KS ogni ora, endpoint REST + SHAP lightweight\" per la demo, incrementare il tasso di consolidamento degli esperimenti e pianificare una revisione degli esperimenti in sospeso e decidere quali abbandonare o approfondire.\n- Chiara Neri: Coordinamento, presentazione finale e demo, comunicazione sul management's request per la demo interattiva del modello, presentazione dei numeri per il budget, suggerire di mettere da parte 20 000 € come buffer e ottimizzare il processo di QA.\n- Elena Verdi: Iscrivere due persone al corso \"Advanced Time-Series Forecasting\" e due persone al workshop \"Explainable AI\", eseguire 50 casi di test tra il 1° e il 5 luglio, ridurre l'impatto del GDPR sulla velocità dei report e definire programma stage e criteri di selezione.\n- Maria Rossi: Riepilogare le spese, gestire la roadmap di release, discutere la revisione delle performance trimestrali del team e organizzare 3 webinar con speaker esterni entro luglio.\n\nNota: ho eliminato le attività duplicate e simili, e ho mantenuto solo le informazioni uniche e rilevanti.\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limiting (429). Retry in 4.43 seconds... (attempt 1/5)\n",
            "Waiting 4.432859512639358 seconds before attempt 2...\n",
            "Rate limiting (429). Retry in 9.67 seconds... (attempt 2/5)\n",
            "Waiting 9.666150772663151 seconds before attempt 3...\n",
            "Rate limiting (429). Retry in 20.19 seconds... (attempt 3/5)\n",
            "Waiting 20.19037954750618 seconds before attempt 4...\n",
            "Results saved in 'meeting_analysis_result.md'\n",
            "\n",
            "Analysis completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final cleaning\n",
        "this section is useful when we are working with big files, that will be divided in many chunks, this may cause some duplications and artifacts\n",
        "what will be managed in this section:\n",
        "- removing all the model artifacts (such as introductive or ending phrases)\n",
        "- removing duplication in titles (derived from chunking division)\n",
        "\n",
        "(This part can be removed, expanded or modified based on personal cases)"
      ],
      "metadata": {
        "id": "1TnEQzAjA2hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_meeting_results(file_path):\n",
        "    \"\"\"\n",
        "    Clean the meeting results file by removing introductive phrases and titles.\n",
        "\n",
        "    Parameters:\n",
        "    file_path(str): File to clean path.\n",
        "\n",
        "    Returns:\n",
        "    str: cleaned file.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Opens file\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in reading file: {e}\")\n",
        "        return file_path\n",
        "\n",
        "    print(f\"Cleaning of file: {file_path}\")\n",
        "\n",
        "    # Removing introductive phrases like \"Ecco il riassunto...\"\n",
        "    content = re.sub(r'Ecco (?:il|l\\').*?:', '', content)\n",
        "\n",
        "    # Removing repeated titles like \"Riassunto del Meeting:\" and \"Attività Identificate:\" that are not preceded by \"#\" (very risky, you have to trust the output of your model)\n",
        "    content = re.sub(r'(?<!#)Riassunto del Meeting:', '', content)\n",
        "    content = re.sub(r'(?<!#)Attività Identificate:', '', content)\n",
        "\n",
        "    # Standardization ( - instead of *)\n",
        "    content = content.replace('* ', '- ')\n",
        "\n",
        "    # Rmoving multiple empty lines\n",
        "    content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
        "\n",
        "    # Saving results in a new file, to avoid loss of information if something's wrong with the cleaning\n",
        "    output_file = file_path.replace('.md', '_clean.md')\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        print(f\"File cleaned and saved as: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore during file saving: {e}\")\n",
        "        return file_path\n",
        "\n",
        "    return output_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function calling\n",
        "print(\"\\nCleaning output file...\")\n",
        "clean_file = clean_meeting_results(\"meeting_analysis_result.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ3iL1AAA2TQ",
        "outputId": "504bb69b-d6bf-41f9-b51d-a8114f679c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaning output file...\n",
            "Cleaning of file: meeting_analysis_result.md\n",
            "File cleaned and saved as: meeting_analysis_result_clean.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- reunite people with same name in task definition, in order to have a more clear and simple task list   \n",
        "(Attention to homonyms!)"
      ],
      "metadata": {
        "id": "hscCIJwRCLQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def organize_tasks_by_person(file_path):\n",
        "    \"\"\"\n",
        "    Read a markdown file and organize tasks by person.\n",
        "\n",
        "    Parameters:\n",
        "    file_path(str): Markdown file path.\n",
        "\n",
        "    Returns:\n",
        "    str: Re-organized file.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Opens file\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during file reading: {e}\")\n",
        "        return file_path\n",
        "\n",
        "    print(f\"Reorganization of tasks in file: {file_path}\")\n",
        "\n",
        "    # Dividing content in sections (resume and task )\n",
        "    sections = re.split(r'(## Riassunto del Meeting|## Attività Identificate)', content)\n",
        "\n",
        "    # resume stays the same\n",
        "    new_content = sections[0]  # Prima parte (titolo principale)\n",
        "\n",
        "    for i in range(1, len(sections)):\n",
        "        if \"## Riassunto del Meeting\" in sections[i]:\n",
        "            new_content += sections[i] + sections[i+1]  # Adding header and summary content\n",
        "        elif \"## Attività Identificate\" in sections[i]:\n",
        "            # Task section processing\n",
        "            tasks_section = sections[i]  # header\n",
        "            if i+1 < len(sections):\n",
        "                tasks_content = sections[i+1]  # task content\n",
        "                # Task extraction\n",
        "                tasks = re.findall(r'- (.*?)(?=\\n- |\\n\\n|\\Z)', tasks_content, re.DOTALL)\n",
        "\n",
        "                # Dictinary to group tasks per person\n",
        "                person_tasks = defaultdict(list)\n",
        "\n",
        "                for task in tasks:\n",
        "                    # Separing name from task description\n",
        "                    match = re.match(r'(.*?):(.*)', task)\n",
        "                    if match:\n",
        "                        person = match.group(1).strip()\n",
        "                        task_desc = match.group(2).strip()\n",
        "                        person_tasks[person].append(task_desc)\n",
        "\n",
        "                # Task section in new format\n",
        "                reorganized_tasks = \"\\n\\n\"\n",
        "                for person, tasks_list in person_tasks.items():\n",
        "                    reorganized_tasks += f\"{person}:\\n\"\n",
        "                    for task in tasks_list:\n",
        "                        reorganized_tasks += f\"- {task}\\n\"\n",
        "                    reorganized_tasks += \"\\n\"\n",
        "\n",
        "                # Adding new section\n",
        "                new_content += tasks_section + reorganized_tasks\n",
        "\n",
        "    # Removing multiple empty lines\n",
        "    new_content = re.sub(r'\\n{3,}', '\\n\\n', new_content)\n",
        "\n",
        "    # Saving results in a new file, to avoid loss of information if something's wrong with the re-organization\n",
        "    output_file = file_path.replace('.md', '_organized.md')\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(new_content)\n",
        "        print(f\"Reorganized file saved as: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during file saving: {e}\")\n",
        "        return file_path\n",
        "\n",
        "    return output_file\n",
        "\n",
        "\n",
        "\n",
        "# Function calling\n",
        "print(\"\\nTasks reorganization...\")\n",
        "organize_tasks_by_person(clean_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "2q1E6qlaCSIJ",
        "outputId": "6f2c5a04-3785-4f44-ecc1-43bcf2ea2f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tasks reorganization...\n",
            "Reorganization of tasks in file: meeting_analysis_result_clean.md\n",
            "Reorganized file saved as: meeting_analysis_result_clean_organized.md\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'meeting_analysis_result_clean_organized.md'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "In this notebook we created a simple program that uses a free LLM to generate meeting summaries and tasks extraction for every people involved.\n",
        "\n",
        "The solution is not perfect, it works well with this data, I suggest to try it with more different data and see if there are some critical issues\n",
        "\n",
        "## Next Steps\n",
        "- Using more powerful LLMs to see the difference in quality and in time response\n",
        "- Compare LLM results with real summaries and tasks extraction results (ground truth), to see if this simple solution can substitute handwritten notes, or at least help\n",
        "- Try to implement a RAG system with LangChain, trying to make questions and receive informations about past meeetings\n",
        "\n"
      ],
      "metadata": {
        "id": "mV7X8phu7iOb"
      }
    }
  ]
}